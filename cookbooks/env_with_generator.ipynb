{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp9lQNIvUg4d"
      },
      "source": [
        "# Working with the Loong environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V3aV16AmY0K"
      },
      "source": [
        "You can also check this cookbook in colab [here](https://colab.research.google.com/drive/1RbNiZMcn5eW_lwwJ4uGZfWKcYIsR5p_4?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvHRdXwflAz-"
      },
      "source": [
        "<div class=\"align-center\">\n",
        "  <a href=\"https://www.camel-ai.org/\"><img src=\"https://i.postimg.cc/KzQ5rfBC/button.png\"width=\"150\"></a>\n",
        "  <a href=\"https://discord.camel-ai.org\"><img src=\"https://i.postimg.cc/L4wPdG9N/join-2.png\"  width=\"150\"></a></a>\n",
        "  \n",
        "‚≠ê <i>Star us on [*Github*](https://github.com/camel-ai/camel), join our [*Discord*](https://discord.camel-ai.org) or follow our [*X*](https://x.com/camelaiorg)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtQdrgZNUg4e"
      },
      "source": [
        "The Loong *environment* is a unified interface that can be used for Synthetic Data Generation, RL training and Benchmarking agents. It integrates all the primitives that we implemented at CAMEL to provide a nice interface for developers and researchers. In this cookbook, we will explain how to initialize a *Single Step Environment* to generate synthetic data. More cookbooks about RL training and how to customize the environment are coming soon.\n",
        "\n",
        "This type of environment is called a *single step* environment, because the agent only does one step. It gets a question sampled from the dataset (the initial state / observation) and then answers. The answer is then scored according to the reward function. Recently, rules-based reward functions, i.e. functions without any learnable parameters, have been successfully used to do RL with LLMs as as policy.\n",
        "\n",
        "Since many RL algorithms (such as GRPO) need multiple rollouts at each step, batching is important to guarantee concurrency / parallelism. Our `SingleStepEnv` supports batching (both `reset` and `step`), but for the sake of simplicity, we will not use batching for this cookbook. We will soon release another cookbook dedicated to batching.\n",
        "\n",
        "First, we have to load a dataset from which we will sample questions. The dataset can be either a `StaticDataset`, which is finite, or it can be a `BaseGenerator`, which is an infinite supply of question - answer pairs, synthetically generated in some way, depending on the implementation. To seed the generative process of the `BaseGenerator`, we need to seed it with a *seed dataset*. Each generator uses the seed dataset it was initialized with to generate new data.\n",
        "\n",
        "In this cookbook, we will use the `FewShotGenerator`, which will generate new data points by doing simple few-shot prompting, using random data points from the seed dataset as examples.\n",
        "\n",
        "A seed dataset can easily be thought of as a type of `StaticDataset`, so let's initialize our seed dataset as such a `StaticDataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vewJOQqLUh2i"
      },
      "outputs": [],
      "source": [
        "# üêç Install in editable mode with dependencies\n",
        "!pip install \"git+https://github.com/camel-ai/camel.git@bec98152d3df3dd1731b78208608b4a9438a010e#egg=camel-ai[all]\"\n",
        "# ‚¨ÖÔ∏è Return to notebook root\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZdba1LJUg4e"
      },
      "outputs": [],
      "source": [
        "from camel.datasets import StaticDataset\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset_dict = load_dataset(\"camel-ai/loong\")\n",
        "dataset = dataset_dict[\"train\"].filter(lambda example: example['source_type'] == 'graph_discrete_math')\n",
        "\n",
        "seed_dataset = StaticDataset(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at an example data point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l7UHBUKY-rJ",
        "outputId": "783675c7-2912-4578-a84f-e55b6f7ac487"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataPoint(question='Given an undirected path graph with 10 vertices, what is the largest independent node set and the list of maximal cliques that can be obtained by repeatedly removing cliques from the graph? Return the result as a 2-tuple, i.e., (largest_independent_node_set, list_of_maximal_cliques), where the first element is a set of nodes in sorted order and the second element is a list of maximal cliques in sorted order (each clique is represented as a set of nodes).', final_answer='({0, 2, 4, 6, 9}, [{0, 1}, {2, 3}, {4, 5}, {6, 7}, {8, 9}])', rationale='import networkx as nx\\n\\nG = nx.path_graph(10)\\nprint(nx.approximation.clique_removal(G))', metadata=None)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = seed_dataset[0]\n",
        "\n",
        "print(f\"Question: {example.question}\")\n",
        "print(f\"Final Answer: {example.final_answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h-s-T6yUg4f"
      },
      "source": [
        "The `FewShotGenerator` needs a python interpreter to compute a synthetic answer (pseudo ground truth) from the code it generated. For this, let's define a `PythonVerifier`.\n",
        "\n",
        "Note: We will soon use dedicated CAMEL-based code interpreters instead of repurposing our Python verifier for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wrkXNgTHUg4f"
      },
      "outputs": [],
      "source": [
        "from camel.verifiers import PythonVerifier\n",
        "from camel.agents import ChatAgent\n",
        "from camel.extractors import BaseExtractor, BoxedStrategy\n",
        "\n",
        "interpreter = PythonVerifier(required_packages=[\"numpy\", \"networkx\"])\n",
        "await interpreter.setup(uv=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0LJzJHxUg4f"
      },
      "source": [
        "Lastly, we need a model backend for the generation agent. Let's use the `ModelFactory` to create one.\n",
        "\n",
        "Note: We use GPT-4o mini as a default here, hence we load our OpenAI API key. Feel free to use other models!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iz_GyOdTUg4f",
        "outputId": "697c6212-8f05-489b-92c2-7459f6823af4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your API key: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "openai_api_key = getpass('Enter your API key: ')\n",
        "os.environ[\"OPENAI_API_KEY\"] = openai_api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zYpsSJ1HUg4g"
      },
      "outputs": [],
      "source": [
        "from camel.models import ModelFactory\n",
        "from camel.types import ModelPlatformType, ModelType\n",
        "from camel.configs import ChatGPTConfig\n",
        "from camel.datasets import FewShotGenerator\n",
        "\n",
        "model = ModelFactory.create(\n",
        "    model_platform=ModelPlatformType.OPENAI,\n",
        "    model_type=ModelType.GPT_4O_MINI,\n",
        "    model_config_dict=ChatGPTConfig().as_dict(),\n",
        ")\n",
        "\n",
        "# Note: When the generator is exhausted, it will create 20 new datapoints by default\n",
        "# To save money on the API, let's set this number to 2 instead, so we don't generate more than we need.\n",
        "generator = FewShotGenerator(\n",
        "    puffer=2, seed_dataset=seed_dataset, verifier=interpreter, model=model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrNvc945efey"
      },
      "source": [
        "Let's next create a verifier that extracts content inside a `\\boxed{...}` from the llm response and compares it semantically to the reference answer.\n",
        "\n",
        "Since we want Loong to be flexible, we utilize built a dedicated extraction module that defines how to parse the llm response and extract the relevant portion that we want to compare. A dedicated cookbook on how to use it is coming soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tgjrKwl0eekB"
      },
      "outputs": [],
      "source": [
        "from camel.verifiers import PythonVerifier\n",
        "from camel.agents import ChatAgent\n",
        "from camel.extractors import BaseExtractor, BoxedStrategy\n",
        "\n",
        "# Initialize extractor\n",
        "extractor = BaseExtractor([[BoxedStrategy()]])\n",
        "\n",
        "\n",
        "verifier = PythonVerifier(extractor=extractor, required_packages=[\"numpy\", \"networkx\"])\n",
        "await verifier.setup(uv=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQFa-ZJoUg4g"
      },
      "source": [
        "Now that our generator and verifier are all set up, let's create a `SingleStepEnv` with it.\n",
        "\n",
        "We can then call `env.reset()` to sample the underlying generator, which returns that question as an observation. We can then feed this observation into the CoT agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL81svP-Ug4g",
        "outputId": "a727dc7b-5ad6-470c-e791-57f1ddcf6dd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:camel.camel.environments.single_step:reset() called on un-setup environment. Setting up...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "question='In a complete bipartite graph K(3,3), what are the edges of the graph represented as a list of tuples, where each tuple represents an edge between two nodes?' context={} metadata={}\n"
          ]
        }
      ],
      "source": [
        "from camel.environments import Action, SingleStepEnv\n",
        "\n",
        "env = SingleStepEnv(generator, verifier)\n",
        "\n",
        "obs = await env.reset(seed=42)\n",
        "\n",
        "print(f\"Observation: {obs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jTmV405Ug4g"
      },
      "source": [
        "The agent would then process this observation and select an action, which it would feed into the `step` function, which feeds it back into the environment. More specifically, it feeds it back into the verifier, which then returns a reward based on whether the llm response and reference answer are aligned or not.\n",
        "\n",
        "Let's first define a CAMEL agent and feed it the observation. Afterwards, we use the `step` function of the environment to get a reward."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hGC17YF8Ug4g"
      },
      "outputs": [],
      "source": [
        "agent = ChatAgent(model=model)\n",
        "\n",
        "USER_PROMPT = r\"\"\"\n",
        "You are an agent designed to answer mathematical questions with clarity and precision. Your task is to provide a step-by-step explanation for\n",
        "any mathematical problem posed by the user, ensuring the response is easy to follow. Adhere to these guidelines:\n",
        "Analyze the mathematical question carefully and break down the solution process into clear, logical steps.\n",
        "Use natural language to explain each step, incorporating LaTeX notation (e.g., $x + 2$)\n",
        "for mathematical expressions when helpful. Conclude your response with the final answer enclosed\n",
        "in a LaTeX \\boxed{} environment (e.g., \\boxed{5}).\n",
        "Place this at the end of your explanation as a standalone statement.\n",
        "It should be a Python expression, for example \"[1, 2, 3]\" for a list.\n",
        "\n",
        "The question you should answer is: \"\"\"\n",
        "\n",
        "response = agent.step(USER_PROMPT + obs.question).msgs[0].content\n",
        "\n",
        "next_obs, reward, done, info = await env.step(Action(llm_response=response))\n",
        "\n",
        "agent.reset()\n",
        "\n",
        "print(f\"Is the episode done?: {done}\")\n",
        "print(f\"Next Observation: {next_obs}\")\n",
        "print(f\"Reward: {reward}\")\n",
        "print(f\"Info: {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the `step` function does exactly what you would expect it to do, if you know how Gym works.\n",
        "\n",
        "`done` is always `True` in this case, since we are in a *single step* environment. This is also the reason that `next_obs` is a placeholder observation, as there is no next observation for this episode.\n",
        "\n",
        "`reward` is the **total** reward for this action. By default, we only use an *accuracy reward*, i.e. $0$ if verifier returns that llm response and synthetic answer are not the same and $10$ otherwise. The accuracy reward can be manually set to any number by simply overriding the attribute of the environment (e.g. `env.ACCURACY_REWARD = 1`). We will add a cookbook soon that shows how to create a custom reward by extending `SingleStepEnv`.\n",
        "\n",
        "Finally, `info` is a dict containing a lot of information about the specifc interaction with the environment. For example, it contains a dict listing the different components of the total reward."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01crFDLvbNCz"
      },
      "source": [
        "### Environment Loop\n",
        "\n",
        "Let's look at how this would look like in a typical loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv2EM-HqbPRn",
        "outputId": "f71002fb-ccc4-4c9e-9323-179445d02dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reward at step 0: 10.0\n",
            "Reward at step 1: 10.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(2):\n",
        "  obs = await env.reset()\n",
        "  response = agent.step(USER_PROMPT + obs.question).msgs[0].content\n",
        "\n",
        "  next_obs, reward, done, info = await env.step(Action(llm_response=response))\n",
        "  print(f\"Reward at step {i}: {reward}\")\n",
        "  agent.reset() # to clear context window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In practice, the agent's model backend would point at an inference engine like vllm or SGLang. After each `step` call (or batches thereof), we would feed the reward for the action into a training framework like *veRL* or *HuggingFace TRL*. These would update the model backend that the agent points to, such that after every training step, the new iteration of the model is used for choosing an action."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Naturally, this setup may seem overkill. In the near future, we will explore multi-step environments like Chess, Go and GAIA, which is why we created this framework to unify RL training. We will work in the coming weeks to make it more efficient and release educational material on how to use it.\n",
        "\n",
        "We are looking forward to see what you build using the Loong environment! Feel free to share it with us on ùïè."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
